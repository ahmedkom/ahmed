import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import time
import re

class SeriesScraper:
    def __init__(self):
        self.base_url = "https://ak.sv"
        self.base_series_url = "https://ak.sv/series?section=29&category=87&rating=0&year=2026&language=1&formats=0&quality=0"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Referer': 'https://ak.sv/',
        }
        self.data_file = 'series_data.json'
        self.stats_file = 'stats.json'
        
    def fetch_page(self, url):
        """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©"""
        try:
            print(f"ğŸ“¡ Ø¬Ù„Ø¨: {url}")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©: {e}")
            return None
    
    def extract_series_from_page(self, html, page_num):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
        soup = BeautifulSoup(html, 'html.parser')
        series_list = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª - Ø­Ø³Ø¨ Ù‡ÙŠÙƒÙ„ Ù…ÙˆÙ‚Ø¹ ak.sv
        series_cards = soup.find_all('div', class_='MovieBlock') or \
                      soup.find_all('div', class_='SeriesCard') or \
                      soup.find_all('article', class_='movie-item') or \
                      soup.find_all('div', class_='col-lg-2 col-md-3 col-sm-4 col-6') or \
                      soup.find_all('div', class_='Thumb--GridItem')
        
        print(f"  ğŸ“„ ØµÙØ­Ø© {page_num}: ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(series_cards)} Ø¹Ù†ØµØ±")
        
        for card in series_cards:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø³Ù„Ø³Ù„
                link_tag = card.find('a')
                if not link_tag:
                    continue
                    
                series_url = link_tag.get('href', '')
                if series_url and not series_url.startswith('http'):
                    series_url = self.base_url + series_url
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø³Ù„Ø³Ù„
                title_tag = card.find('h3') or card.find('h4') or card.find('h5') or \
                           card.find('div', class_='Title') or card.find('span', class_='name')
                title = title_tag.text.strip() if title_tag else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©
                img_tag = card.find('img')
                img_url = img_tag.get('src', '') if img_tag else ''
                if img_url and not img_url.startswith('http'):
                    img_url = self.base_url + img_url
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
                title = re.sub(r'\d+\s*:\s*', '', title)  # Ø´ÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø²ÙŠ "45 :"
                title = re.sub(r'^\d+\s*', '', title)     # Ø´ÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
                
                series_list.append({
                    'id': f"page{page_num}_{len(series_list)}",
                    'title': title.strip(),
                    'url': series_url,
                    'image': img_url,
                    'year': '2026',
                    'episodes': [],
                    'last_updated': datetime.now().isoformat(),
                    'source_page': page_num,
                    'status': 'Ù…Ø³ØªÙ…Ø±'
                })
                
            except Exception as e:
                print(f"    âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³Ù„Ø³Ù„: {e}")
                continue
        
        return series_list
    
    def scrape_all_pages(self, start_page=1, end_page=5):
        """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù…Ù† ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª"""
        all_series = []
        
        for page in range(start_page, end_page + 1):
            # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©
            if page == 1:
                url = self.base_series_url
            else:
                url = f"{self.base_series_url}&page={page}"
            
            print(f"\nğŸ“‘ Ø¬Ù„Ø¨ ØµÙØ­Ø© {page}/{end_page}")
            html = self.fetch_page(url)
            
            if html:
                page_series = self.extract_series_from_page(html, page)
                all_series.extend(page_series)
                print(f"  âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(page_series)} Ù…Ø³Ù„Ø³Ù„ ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}")
            else:
                print(f"  âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© {page}")
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¨ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª Ø¹Ø´Ø§Ù† Ù…Ø§ Ù†Ø¶ØºØ·Ø´ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±
            if page < end_page:
                print("  â³ Ø§Ù†ØªØ¸Ø§Ø± 2 Ø«Ø§Ù†ÙŠØ©...")
                time.sleep(2)
        
        return all_series
    
    def extract_episodes(self, series_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù…Ø³Ù„Ø³Ù„"""
        print(f"  ğŸ“¥ Ø¬Ù„Ø¨ Ø­Ù„Ù‚Ø§Øª...")
        html = self.fetch_page(series_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        episodes = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø§Øª
        episode_elements = soup.find_all('a', href=re.compile(r'/episode/')) or \
                          soup.find_all('div', class_='Episode') or \
                          soup.find_all('li', class_='episode-item') or \
                          soup.find_all('a', class_='watch-episode')
        
        for i, episode in enumerate(episode_elements[:30], 1):  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 30 Ø­Ù„Ù‚Ø©
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø©
                link = episode if episode.name == 'a' else episode.find('a')
                if not link:
                    continue
                    
                episode_url = link.get('href', '')
                if episode_url and not episode_url.startswith('http'):
                    episode_url = self.base_url + episode_url
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø©
                watch_url = self.extract_watch_url(episode_url)
                
                episodes.append({
                    'number': i,
                    'title': f"Ø§Ù„Ø­Ù„Ù‚Ø© {i}",
                    'url': episode_url,
                    'watch_url': watch_url,
                    'added_date': datetime.now().isoformat()
                })
                
                if watch_url:
                    print(f"    âœ… Ø§Ù„Ø­Ù„Ù‚Ø© {i}: ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø·")
                else:
                    print(f"    âš ï¸ Ø§Ù„Ø­Ù„Ù‚Ø© {i}: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø§Ø¨Ø· Ù…Ø´Ø§Ù‡Ø¯Ø©")
                    
                time.sleep(0.3)  # ØªØ¬Ù†Ø¨ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±
                
            except Exception as e:
                print(f"    âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ù„Ù‚Ø© {i}: {e}")
                continue
        
        return episodes
    
    def extract_watch_url(self, episode_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±"""
        html = self.fetch_page(episode_url)
        if not html:
            return None
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· .m3u8
        m3u8_pattern = r'https?://[^\s"\']+\.m3u8[^\s"\']*'
        m3u8_matches = re.findall(m3u8_pattern, html)
        
        if m3u8_matches:
            return m3u8_matches[0]
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· .mp4
        mp4_pattern = r'https?://[^\s"\']+\.mp4[^\s"\']*'
        mp4_matches = re.findall(mp4_pattern, html)
        
        if mp4_matches:
            return mp4_matches[0]
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† iframe
        soup = BeautifulSoup(html, 'html.parser')
        iframe = soup.find('iframe')
        if iframe and iframe.get('src'):
            return iframe.get('src')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† video source
        video = soup.find('video')
        if video:
            source = video.find('source')
            if source and source.get('src'):
                return source.get('src')
        
        return None
    
    def load_existing_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©"""
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {'series': [], 'last_update': None, 'total_series': 0, 'total_pages': 0}
        return {'series': [], 'last_update': None, 'total_series': 0, 'total_pages': 0}
    
    def save_data(self, data):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù JSON"""
        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(data['series'])} Ù…Ø³Ù„Ø³Ù„ ÙÙŠ {self.data_file}")
    
    def save_stats(self, data, pages_scraped):
        """Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        total_episodes = sum(len(s['episodes']) for s in data['series'])
        episodes_with_links = sum(1 for s in data['series'] for e in s['episodes'] if e.get('watch_url'))
        
        stats = {
            'total_series': len(data['series']),
            'total_episodes': total_episodes,
            'episodes_with_links': episodes_with_links,
            'pages_scraped': pages_scraped,
            'last_update': data['last_update'],
            'source_url': self.base_series_url,
            'next_update': 'ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª'
        }
        
        with open(self.stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙÙŠ {self.stats_file}")
    
    def merge_series_data(self, old_data, new_series_list):
        """Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        if not old_data.get('series'):
            old_data['series'] = []
        
        old_series_dict = {s['url']: s for s in old_data['series']}
        new_count = 0
        updated_count = 0
        
        for new_series in new_series_list:
            if new_series['url'] in old_series_dict:
                # Ù…Ø³Ù„Ø³Ù„ Ù…ÙˆØ¬ÙˆØ¯ - Ù†ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                old_series = old_series_dict[new_series['url']]
                old_episodes = {e['number']: e for e in old_series.get('episodes', [])}
                
                new_episodes = []
                for ep in new_series['episodes']:
                    if ep['number'] not in old_episodes:
                        new_episodes.append(ep)
                
                if new_episodes:
                    old_series['episodes'].extend(new_episodes)
                    old_series['last_updated'] = datetime.now().isoformat()
                    updated_count += len(new_episodes)
            else:
                # Ù…Ø³Ù„Ø³Ù„ Ø¬Ø¯ÙŠØ¯
                old_data['series'].append(new_series)
                new_count += 1
        
        old_data['last_update'] = datetime.now().isoformat()
        old_data['total_series'] = len(old_data['series'])
        
        print(f"ğŸ“Š Ù…Ù„Ø®Øµ: {new_count} Ù…Ø³Ù„Ø³Ù„ Ø¬Ø¯ÙŠØ¯ØŒ {updated_count} Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©")
        
        return old_data
    
    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª"""
        print("=" * 60)
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© 2026 Ù…Ù† ak.sv")
        print("=" * 60)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª
        start_page = 1
        end_page = 5
        print(f"ğŸ“‘ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø§Øª Ù…Ù† {start_page} Ø¥Ù„Ù‰ {end_page}")
        print("-" * 60)
        
        # Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù…Ù† ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª
        all_series = self.scrape_all_pages(start_page, end_page)
        
        print("\n" + "=" * 60)
        print(f"ğŸ“º Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù‚Ø¨Ù„ Ø¬Ù„Ø¨ Ø§Ù„Ø­Ù„Ù‚Ø§Øª: {len(all_series)}")
        print("=" * 60)
        
        # Ø¬Ù„Ø¨ Ø­Ù„Ù‚Ø§Øª ÙƒÙ„ Ù…Ø³Ù„Ø³Ù„ (Ø£ÙˆÙ„ 10 Ù…Ø³Ù„Ø³Ù„Ø§Øª ÙÙ‚Ø· Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
        # Ù„Ùˆ Ø¹Ø§ÙˆØ² Ø§Ù„ÙƒÙ„ØŒ ØºÙŠØ± range(len(all_series))
        max_series_to_process = min(10, len(all_series))  # Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ Ù‡Ø¬ÙŠØ¨ Ø£ÙˆÙ„ 10 Ø¨Ø³
        print(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø­Ù„Ù‚Ø§Øª Ø£ÙˆÙ„ {max_series_to_process} Ù…Ø³Ù„Ø³Ù„...")
        
        for i in range(max_series_to_process):
            series = all_series[i]
            print(f"\n[{i+1}/{max_series_to_process}] ğŸ“º {series['title'][:50]}")
            series['episodes'] = self.extract_episodes(series['url'])
            print(f"   âœ… {len(series['episodes'])} Ø­Ù„Ù‚Ø©")
            time.sleep(1)  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ¯Ù…Ø¬Ù‡Ø§
        print("\n" + "-" * 60)
        print("ğŸ”„ Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        old_data = self.load_existing_data()
        updated_data = self.merge_series_data(old_data, all_series)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.save_data(updated_data)
        self.save_stats(updated_data, end_page)
        
        print("\n" + "=" * 60)
        print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª: {updated_data['total_series']}")
        print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ù„Ù‚Ø§Øª: {sum(len(s['episodes']) for s in updated_data['series'])}")
        print("=" * 60)
        print("â±ï¸  Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø§Ø¯Ù…: Ø¨Ø¹Ø¯ 6 Ø³Ø§Ø¹Ø§Øª")
        print("=" * 60)

if __name__ == "__main__":
    scraper = SeriesScraper()
    scraper.run()
